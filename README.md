# Train Large Language Models on Multiple GPUs

## Training Parallelism

### Data Parallelism

### Model Parallelism

### Pipeline Parallelism

### Tensor Parallelism

#### References

[1] Weng, Lilian. (Sep 2021). How to train really large models on many GPUs? Lilâ€™Log. https://lilianweng.github.io/posts/2021-09-25-train-large/.

[2] https://openai.com/blog/techniques-for-training-large-neural-networks/

[3] https://huggingface.co/docs/transformers/perf_train_gpu_many

[4] https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
