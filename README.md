# Techniques for Training Large Language Models on Multiple GPUs

There are several parallelism paradigms to enable model training across multiple GPUs.

<img width="689" alt="Screenshot 2022-08-31 at 1 50 32 PM" src="https://github.com/aniket-mish/parallelism/assets/71699313/a5925986-5c7f-4875-ba5a-9678ed24d480">

## PyTorch Distributed Data Parallel



## References

[1] [How to train really large models on many GPUs? Lilâ€™Log](https://lilianweng.github.io/posts/2021-09-25-train-large/)

[2] [Techniques for training large neural networks](https://openai.com/blog/techniques-for-training-large-neural-networks/)

[3] [Efficient Training on Multiple GPUs](https://huggingface.co/docs/transformers/perf_train_gpu_many)

[4] [Getting started with distributed data parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
